{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letter Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will use neural networks (MLP) for the task of predicting the each of a large number of black-and-white rectangular pixel displays as one of the 26 capital letters in the English alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A: Source Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creator: David J. Slate Odesta Corporation; 1890 Maple Ave; Suite 115; Evanston, IL 60201  \n",
    "Donor: David J. Slate (dave@math.nwu.edu) (708) 491-3867  \n",
    "Date: January, 1991  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B: Relevant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli.  Each stimulus was converted into 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15.  We typically train on the first 16000 items and then use the resulting model to predict the letter category for the remaining 4000.  See the article cited above for more details:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P. W. Frey and D. J. Slate (Machine Learning Vol 6 #2 March 91): \"Letter Recognition Using Holland-style Adaptive Classifiers\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C:Attribute Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Att_Letter.png\" width = \"600\" height = \"300\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code uses Python's `csv` module to load the data and prints the first row and the total number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header line: ['lettr', 'x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx']\n",
      "['T', '2', '8', '3', '5', '1', '8', '13', '0', '6', '6', '10', '8', '0', '8', '0', '8']\n",
      "Total number of rows: 20000\n"
     ]
    }
   ],
   "source": [
    "with open('Letter.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    print(\"Header line: %s\" % next(reader))\n",
    "    annotated_data = [r for r in reader]\n",
    "print(annotated_data[0])\n",
    "print(\"Total number of rows:\", len(annotated_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "Header=['lettr', 'x-box', 'y-box', 'width', 'high', 'onpix', 'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', 'xy2br', 'x-ege', 'xegvy', 'y-ege', 'yegvx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (1 mark) - Class Distribution\n",
    "Print the number of class label lettr (A-E).\n",
    "\n",
    "* A: ?\n",
    "* B: ?\n",
    "* C: ?\n",
    "* D: ?\n",
    "* E: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 789\n",
      "B: 766\n",
      "C: 736\n",
      "D: 805\n",
      "E: 768\n",
      "F: 775\n",
      "G: 773\n",
      "H: 734\n",
      "I: 755\n",
      "J: 747\n",
      "K: 739\n",
      "L: 761\n",
      "M: 792\n",
      "N: 783\n",
      "O: 753\n",
      "P: 803\n",
      "Q: 783\n",
      "R: 758\n",
      "S: 748\n",
      "T: 796\n",
      "U: 813\n",
      "V: 764\n",
      "W: 752\n",
      "X: 787\n",
      "Y: 786\n",
      "Z: 734\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(map(lambda x: x[0], annotated_data))\n",
    "for letter in string.ascii_uppercase:\n",
    "    print(\"%s: %s\" % (letter,counts[letter]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (1 mark) - Check that the distrubution of the class label 'lettr'\n",
    "Split the data into a training set, a dev-test set, and a test set. Use the following ratio for splitting the data:\n",
    "\n",
    "* Training set: 80%\n",
    "* Dev-test set: 10%\n",
    "* Test set: 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random  \n",
    "#random.seed(1234)  \n",
    "#random.shuffle(annotated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.1\n",
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train_test_split should randomly shuffle the data\n",
    "training_set,other_sets = train_test_split(annotated_data,test_size=0.2)\n",
    "dev_test_set,test_set = train_test_split(other_sets, test_size=0.5)\n",
    "\n",
    "print(len(training_set)/len(annotated_data))\n",
    "print(len(dev_test_set)/len(annotated_data))\n",
    "print(len(test_set)/len(annotated_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (1 mark) - Check that the data are balanced\n",
    "Print the percentage of class label lettr (A-E) in each partition, and check that they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1924375\n"
     ]
    }
   ],
   "source": [
    "letter_count = sum(map(lambda x: 1 if x[0] in list(\"ABCDE\") else 0,training_set))\n",
    "average = letter_count/len(training_set)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.181\n"
     ]
    }
   ],
   "source": [
    "letter_count = sum(map(lambda x: 1 if x[0] in list(\"ABCDE\") else 0,dev_test_set))\n",
    "average = letter_count/len(dev_test_set)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2115\n"
     ]
    }
   ],
   "source": [
    "letter_count = sum(map(lambda x: 1 if x[0] in list(\"ABCDE\") else 0,test_set))\n",
    "average = letter_count/len(test_set)\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (3 marks) - Neural Network MLP in Scikit-Learn \n",
    "Train an `sklearn` MLPClassifier with default settings (random_state=0) using the training set and report the accuracy on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['W', '6', '8', '6', '6', '6', '4', '9', '2', '3', '9', '8', '8', '7', '11', '2', '6'], ['G', '4', '6', '6', '6', '6', '7', '7', '5', '4', '7', '7', '8', '7', '10', '6', '8'], ['T', '2', '7', '4', '4', '1', '9', '14', '1', '6', '5', '11', '9', '0', '8', '0', '8'], ['P', '3', '7', '5', '5', '3', '9', '8', '2', '5', '12', '4', '5', '1', '10', '2', '9'], ['G', '6', '6', '7', '8', '3', '8', '5', '8', '9', '6', '5', '10', '2', '8', '6', '11']]\n",
      "[[ 6.  8.  6. ... 11.  2.  6.]\n",
      " [ 4.  6.  6. ... 10.  6.  8.]\n",
      " [ 2.  7.  4. ...  8.  0.  8.]\n",
      " ...\n",
      " [ 7. 12.  7. ...  9.  2.  7.]\n",
      " [ 3.  7.  5. ...  9.  3.  7.]\n",
      " [ 3.  7.  4. ...  9.  9.  8.]]\n",
      "Training set prediction score:\n",
      "0.89825\n",
      "Test set prediction score:\n",
      "0.9075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "training_set_letters = np.array(training_set)[:,0]\n",
    "training_set_data = np.array(training_set)[:,1:].astype(float)\n",
    "\n",
    "clf = MLPClassifier(random_state=0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_set_data,training_set_letters)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "x_test_pred = clf.predict(x_test)\n",
    "print(\"Training set prediction score:\")\n",
    "print(accuracy_score(x_test_pred, y_test))\n",
    "\n",
    "test_set_letters = np.array(test_set)[:,0]\n",
    "test_set_data = np.array(test_set)[:,1:].astype(float)\n",
    "\n",
    "test_pred = clf.predict(test_set_data)\n",
    "print(\"Test set prediction score:\")\n",
    "print(accuracy_score(test_pred, test_set_letters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (8 marks) - Neural Network MLP with Scaling of the Data\n",
    "Neural networks expect all input features to vary in a way, and ideally to have a mean of 0, and a variance of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 compute the mean value per feature on the training set [1 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-box mean: 4.0240625\n",
      "y-box mean: 7.0431875\n",
      "width mean: 5.1286875\n",
      "high mean: 5.38125\n",
      "onpix mean: 3.5175\n",
      "x-bar mean: 6.8875625\n",
      "y-bar mean: 7.515875\n",
      "x2bar mean: 4.6114375\n",
      "y2bar mean: 5.198875\n",
      "xybar mean: 8.2895\n",
      "x2ybr mean: 6.461625\n",
      "xy2br mean: 7.921875\n",
      "x-ege mean: 3.0325625\n",
      "xegvy mean: 8.3413125\n",
      "y-ege mean: 3.7068125\n",
      "yegvx mean: 7.8024375\n"
     ]
    }
   ],
   "source": [
    "for column in range(training_set_data.shape[1]):\n",
    "    print(\"%s mean: %s\" % (Header[column+1],training_set_data[:,column].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 compute the standard deviation of each feature on the training set [1 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-box std: 1.908627385346273\n",
      "y-box std: 3.301487897879341\n",
      "width std: 2.0130206226821796\n",
      "high std: 2.264927910000669\n",
      "onpix std: 2.1904494401834524\n",
      "x-bar std: 2.024764260005038\n",
      "y-bar std: 2.3195900466192296\n",
      "x2bar std: 2.680896619340953\n",
      "y2bar std: 2.373594054250853\n",
      "xybar std: 2.4884613217809917\n",
      "x2ybr std: 2.6347252910645165\n",
      "xy2br std: 2.074010965345892\n",
      "x-ege std: 2.3245703223593277\n",
      "xegvy std: 1.5331970771377532\n",
      "y-ege std: 2.5697477677475953\n",
      "yegvx std: 1.6118022703153603\n"
     ]
    }
   ],
   "source": [
    "for column in range(training_set_data.shape[1]):\n",
    "    print(\"%s std: %s\" % (Header[column+1],training_set_data[:,column].std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 subtract the mean, and scale by inverse standard deviation;  afterward, mean=0 and std=1 [1 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-box mean: -0.0, std: 1.0\n",
      "y-box mean: -0.0, std: 1.0\n",
      "width mean: 0.0, std: 1.0\n",
      "high mean: 0.0, std: 1.0\n",
      "onpix mean: -0.0, std: 1.0\n",
      "x-bar mean: 0.0, std: 1.0\n",
      "y-bar mean: -0.0, std: 1.0\n",
      "x2bar mean: -0.0, std: 1.0\n",
      "y2bar mean: -0.0, std: 1.0\n",
      "xybar mean: -0.0, std: 1.0\n",
      "x2ybr mean: 0.0, std: 1.0\n",
      "xy2br mean: -0.0, std: 1.0\n",
      "x-ege mean: -0.0, std: 1.0\n",
      "xegvy mean: -0.0, std: 1.0\n",
      "y-ege mean: 0.0, std: 1.0\n",
      "yegvx mean: 0.0, std: 1.0\n"
     ]
    }
   ],
   "source": [
    "training_set_data_normalized = training_set_data.copy()\n",
    "for column in range(training_set_data.shape[1]):\n",
    "    array = training_set_data[:,column]\n",
    "    array = (array - array.mean()) / array.std()\n",
    "    training_set_data_normalized[:,column] = array\n",
    "    \n",
    "    print(\"%s mean: %0.1f, std: %0.1f\" % (Header[column+1],array.mean(),array.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 use THE SAME transformation (using training mean and std) on the test set [1 mark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-box mean: -0.0, std: 1.0\n",
      "y-box mean: 0.0, std: 1.0\n",
      "width mean: 0.0, std: 1.0\n",
      "high mean: -0.0, std: 1.0\n",
      "onpix mean: -0.0, std: 1.0\n",
      "x-bar mean: 0.0, std: 1.0\n",
      "y-bar mean: 0.0, std: 1.0\n",
      "x2bar mean: -0.0, std: 1.0\n",
      "y2bar mean: -0.0, std: 1.0\n",
      "xybar mean: -0.0, std: 1.0\n",
      "x2ybr mean: -0.0, std: 1.0\n",
      "xy2br mean: -0.0, std: 1.0\n",
      "x-ege mean: -0.0, std: 1.0\n",
      "xegvy mean: -0.0, std: 1.0\n",
      "y-ege mean: 0.0, std: 1.0\n",
      "yegvx mean: 0.0, std: 1.0\n"
     ]
    }
   ],
   "source": [
    "test_set_data_normalized = test_set_data.copy()\n",
    "for column in range(test_set_data.shape[1]):\n",
    "    array = test_set_data[:,column]\n",
    "    array = (array - array.mean()) / array.std()\n",
    "    test_set_data_normalized[:,column] = array\n",
    "    \n",
    "    print(\"%s mean: %0.1f, std: %0.1f\" % (Header[column+1],array.mean(),array.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Train an `sklearn` MLPClassifier with default settings (random_state=0) using the scaled training set and report the accuracy on the scaled training and scaled test set.  [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set prediction score:\n",
      "0.9425\n",
      "Test set prediction score:\n",
      "0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "clf = MLPClassifier(random_state=0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_set_data_normalized,training_set_letters)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "x_test_pred = clf.predict(x_test)\n",
    "print(\"Training set prediction score:\")\n",
    "print(accuracy_score(x_test_pred, y_test))\n",
    "\n",
    "test_pred = clf.predict(test_set_data_normalized)\n",
    "print(\"Test set prediction score:\")\n",
    "print(accuracy_score(test_pred, test_set_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Increase the number of iterations to 1000 to see whether the optimization has been converged. [2 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set prediction score:\n",
      "0.94575\n",
      "Test set prediction score:\n",
      "0.9455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "clf = MLPClassifier(random_state=0,max_iter=1000)\n",
    "x_train, x_test, y_train, y_test = train_test_split(training_set_data_normalized,training_set_letters)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "x_test_pred = clf.predict(x_test)\n",
    "print(\"Training set prediction score:\")\n",
    "print(accuracy_score(x_test_pred, y_test))\n",
    "\n",
    "test_pred = clf.predict(test_set_data_normalized)\n",
    "print(\"Test set prediction score:\")\n",
    "print(accuracy_score(test_pred, test_set_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (2 marks)-KNN with different k values\n",
    "Training KNN models with different k values (1-10), and then report the best accuracy and its k value on unscaled training/test and scaled  training/test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[code...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 (4 marks) - Analysis of Results\n",
    "Analyse the results of all the classifiers from the previous exercises, and answer these questions. In all answers you must include any code that you need to implement to answer the questions, the output of the code, and an interpretation of the output that shows how it can be used to answer the questions.\n",
    "\n",
    "1. (1 mark) Did you observe any overfitting in any of the classifiers? How did you determine whether they have overfitting?\n",
    "2. (3 marks) Do we have too little training data, or do we have too much training data for these classifiers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your submission should consist of this jupyter notebook with all your code and explanations inserted in the notebook. The notebook should contain the output of the runs so that it can be read by the assessor without needing to run the output.\n",
    "\n",
    "DataCamp: Jupyter Notebook Tutorial: The Definitive Guide. A good overview of Jupyter notebooks, how to install and run them, why they are a good idea, some key features. Click https://www.datacamp.com/community/tutorials/tutorial-jupyter-notebook link to open resource.\n",
    "\n",
    "Late submissions will have a penalty of **4 marks deduction per day late**.\n",
    "\n",
    "Each question specifies a mark. The final mark of the assignment is the sum of all the individual marks, after applying any deductions for late submission.\n",
    "\n",
    "By submitting this assignment you are acknowledging that this is your own work. Any submissions that break the code of academic honesty will be penalised as per the [academic honesty policy](https://staff.mq.edu.au/work/strategy-planning-and-governance/university-policies-and-procedures/policies/academic-honesty)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
